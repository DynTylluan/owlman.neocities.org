<html>
  <head>
<!-- The OwlMan Library is composed of original material and may      -->
<!-- be used aslong as one follows CC BY-NC-SA 3.0                    -->
<!-- https://owlman.neocities.org/library/                            -->
<!--                                                                  -->
<!-- Copyright (C) 2016 Clive "James" Python                          -->
<!-- Some rights reserved                                             -->
<!--                                                                  -->
<!-- E-mail: owlman at protonmail dot com                             -->
<!-- And yeah, I like Rotten Dot Com's Library                        -->
  <link rel="icon" type="image/png" href="https://owlman.neocities.org/img/OM_Logo.gif" />
  <title>Archiving Everything | OWLMAN</title>
    <body background="https://owlman.neocities.org/img/dump/green.png">

<center><BODY BGCOLOR="#cccccc" TEXT="#ffffff" LINK="#ffffff" ALINK="#ffffff" VLINK="#ffffff">
<font color="white"><p><font size="6">Archiving Everything</a>
<font size="5"> <P>By Clive Python | 14jammar<font size="4"><P>

<TABLE WIDTH=750><TD VALIGN=TOP>
<P><img src="https://owlman.neocities.org/library/img/1984.png" align=right hspace=15 vspace=5>
 Nothing lasts forever, that's a motherfucking fact Sunny Jim, but there are 
 some good ways to archive the Web so that it can last just a little longer. 
 Here are some helpful resources to archiving the Web as well as some useful 
 websites that you may use. 
<P>
<h1>What are robots.txt?</h1>
<P>
Before we go over some archive websites, I think it is a good idea to talk about what robots.txt are and how they work.
<P>
For the people who don't know how robots.txt work, here is an example of my own website that might help you better understand... or not...
<P><tt><blockquote>
# robots.txt file for owlman.neocities.org</br>
# Created in the distant future (the year 2000) after</br>
# the robotic uprising of the mid 90's which wiped out all humans.
<P>
User-agent: *</br>
Disallow: /Who_in_Ascii_Art.html</br>
Disallow: /ascii.html</br>
Disallow: /odds/robotstxtexample.txt
</tt></blockquote>
<P>
<!-- BTW, I stoped archiving on /ascii.html and /Who_in_Ascii_Art.html so people's email's would not get spamed, /odds/robotstxtexample.txt is just an example page -->
When broken down we can see that robots can not view the pages <a href="/Who_in_Ascii_Art.html">/Who_in_Ascii_Art.html</A>, 
<a href="/ascii.html">/ascii.html</A> and <a href="/odds/robotstxtexample.txt">/odds/robotstxtexample.txt</A>, becasue of this it means
that The Internet Archive will <a href="https://web.archive.org/save/https://owlman.neocities.org/odds/robotstxtexample.txt">not display this particaler page</a>
<P>
Robots.txt files live on the top level of a website at a URL 
like this: <a href="https://owlman.neocities.org/robots.txt">https://owlman.neocities.org/robots.txt</a>. 
This standard was developed in 1994 to guide search engine crawlers in a variety of ways, including some areas to avoid crawling.
<P>
The robots exclusion standard (also called the robots exclusion protocol or robots.txt protocol) is 
a way of telling Web crawlers and other Web robots which parts of a Web site they can see, in this 
case we are talking about what archive bots can see and archive.
<P>
<h1>The Internet Archvie</h1>
<p>
<tt><a href="https://archive.org/">https://archive.org/</a></tt></br>
<tt><a href="https://web.archive.org/">https://web.archive.org/</a></tt>
<P>

The Internet Archive was founded in 1996 and is one of the most used web archive websites on the 
Net, I would highly rate the site as it is very easy and 
quick to use. The Internet Archive does not just archive websites, but it also has a big array of 
<a href="https://archive.org/details/texts">eBooks</A>,
<a href="https://archive.org/details/movies">movies</a>,
<a href="https://archive.org/details/audio">an audio archive</a>,
<a href="https://archive.org/details/tv">TV news archive</a>,
<a href="https://archive.org/details/software">software collection</a>,
<a href="https://archive.org/details/etree">music</a>,
<a href="https://archive.org/search.php?query=mediatype:collection&sort=-downloads">plus much more</a> for no cost at all for the user.
<P>
One of the downsides to using the website is that it obeys robots.txt. This means if 
someone doesn't want a page archived - <i>or even their whole website</i> - they can choose not to.
<P>
The Internet Archive has two add-ons for <a href="https://chrome.google.com/webstore/detail/wayback-machine/fpnmgdkabkmnadcjpehmlllkndpkmiak">Google Chrome</a> 
and 
<A href="https://addons.mozilla.org/en-US/firefox/addon/wayback-machine_new/?src=userprofile">Firefox</A>, 
both are the same but for different web browsers. 
<P>
<h1>archive.is</h1>
<p>
<tt><a href="https://archive.is/">https://archive.is/</a></tt></br>
<tt><a href="https://archive.fo/">https://archive.fo/</a></tt></br>
<tt><a href="https://archive.today/">https://archive.today/</a></tt></br>
<tt><a href="http://archive.li/">http://archive.li/</a></tt>
<P>
Unlike The Internet Archive archive.is does not obey robots.txt, this is 
because "it is not a free-walking crawler, [archive.is] saves only 
one page acting as a direct agent of the human user".
<P>
In short, this means that you can save web pages that you otherwise 
could not with The Internet Archive. One very big downsid is that you 
can't archive content loaded by Flash, video, audio, PDF 
files, RSS and other XML-pages.
<P>
<h1>HTTrack</h1>
<p>
<tt><a href="http://www.httrack.com/">http://www.httrack.com/</a></tt>
<P>
Are you really into archiving websites that you have got to the point where you 
download an entire website? Well, if you have, HTTrack is a good place to start. 
<P>
HTTrack is very easy to use, however - and this is a very big '<i>however</i>', to 
properly download websites, you need to be connected to the Web for what 
could be a long time depending on the size of the website. For example, 
when downloading this website 
(<i><a href="https://owlman.neocities.org/">https://owlman.neocities.org/</A></i>) 
it may only 
take one hour or so, but for websites such as Textfiles.com it can take 
up to three plus days.
<P>
One of the biggest downsides to it is that for some reason, some websites 
won't download, like, at all so you're fucked. The only way around this 
would be to manually download the site, but depending on what site it 
is, this can be a massive pain in the arse
<P>
<h1>GifCities</h1>
<P>
<tt><a href="https://gifcities.org/">https://gifcities.org/</A></tt>
<P>
 Owned by The Internet Archive, GifCities is basically a search engine for 
 .gif from GeoCitie websites. There's really not much to say about this 
 website, it's just a cool search engine that was made in celebration of 
 The Internet Archive becoming 20 years old. So if you want to search for 
 some <a href="https://gifcities.org/?q=b3ta">kewl stuff</a>, look no further.
<P>
<h1>textfiles.com</h1>
<p>
<tt><a href="http://www.textfiles.com/">http://www.textfiles.com/</a></tt>
<P>
Setup in 1998 by <a href="https://owlman.neocities.org/ascii.html#textfiles.com">Jason Scott</a>, textfiles.com is dedicated to 
preserving the digital documents that contain the history of 
the BBS world and various subcultures. The site categorises 
and stores thousands of ASCII files. It focuses on text files 
from the 1980s, but also contains some older files and some 
that were created well into the 1990s... 
<i>or at least according to Wikipedia...</i>
<P>
<h1>ReoCities</h1>
<p>
<tt><a href="http://www.reocities.com/">http://www.reocities.com/</a></tt>
<P>
 Born in the ashes of GeoCities, ReoCities' aim is to archive Yahoo's 
 now-dead website host service after they pulled the plug on it in 
 2009. One annoying thing about the website is that whenever you 
 view an archive, say <a href="http://reocities.com/SouthBeach/Palms/2115/">http://reocities.com/SouthBeach/Palms/2115/</A> 
 it has a big FUXKING banner at the top of the page that says "<tt>If 
 you like the reocities.com project you can donate bitcoins to: 
 1E8rQq9cmv95CrdrLmqaoD6TErUFKok3bF</tt>", yeah, thanks, lads. 
<P>
<h1>Google Groups</h1>
<P>
<tt><a href="https://groups.google.com">https://groups.google.com</a></tt>
<P>
Helpful archive for Usenet groups from the <i>nice</i> people at Google. 
The website has a large back catalogue of newsgroups started in 
1995 when it was owned by The Deja News Research Service. This 
archive was acquired by Google in 2001. The Google Groups archive 
of Usenet newsgroup postings dates back to 1981. 
<P>
<h1>OnionLink</h1>
<P>
<tt><a href="http://onion.link/">http://onion.link/</a></tt>
<P>
Website that allows you to view Deep Web sites on a normal browser such as FireFox. If you see a Deep Web URL, simply put .link at the end, example;
<P>
<tt>http://3g2upl4pq6kufc4m.onion <i>is now going to be</i> http://3g2upl4pq6kufc4m.onion.link</tt>
<P>
<tt>(<a href="https://web.archive.org/web/20160714152313/https://en.wikipedia.org/wiki/DuckDuckGo">DDG's Tor URL, BTW</A>)</tt>
<P>
<b>But for the love of God, make sure the link is safe to view.</b>
<P>
<hr></hr>
<PRE>
<tt>Written by Clive "James" Python, 12/09/17.</tt>

<a href="https://owlman.neocities.org/library/archive.html">https://owlman.neocities.org/library/archive.html</a>
<a href="https://web.archive.org/web/*/https://owlman.neocities.org/library/archive.html*">https://web.archive.org/web/*/https://owlman.neocities.org/library/archive.html*</a></PRE>
<P>
<font color="black"><center>&#x2605;</center></font color>
  </body>
</html>